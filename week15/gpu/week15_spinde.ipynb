{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00515355-73a6-4d0b-9c75-bdc0d5add221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ihome/xli/sek188/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/ihome/crc/install/pytorch/2.0.1/python3.10/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/ihome/crc/install/pytorch/2.0.1/python3.10/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/ihome/crc/install/pytorch/2.0.1/python3.10/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/ihome/crc/install/pytorch/2.0.1/python3.10/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /ihome/xli/sek188/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /ihome/xli/sek188/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25ee2f75-9158-4a2e-9d1d-f5eb6b13b9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /ihome/xli/sek188/.local/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: joblib in /ihome/xli/sek188/.local/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Collecting click\n",
      "  Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /ihome/xli/sek188/.local/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Installing collected packages: click, nltk\n",
      "Successfully installed click-8.2.1 nltk-3.9.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64b06e1c-4609-4b4e-944f-0930e3f29274",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mediabiasgroup/da-roberta-babe-ft\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb59c70c-72fa-4f72-9bfd-e7cfda53f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dataset\n",
    "wdf = pd.read_csv('combined_wide_form_jul30.csv')\n",
    "wdf = wdf[['article_set_unique_id', 'source_url', 'title', 'text', 'human_clean_title', 'human_clean_text', 'pull']]\n",
    "wdf['clean'] = 'HEADLINE: ' + wdf['human_clean_title'] + \"ARTICLE\" + wdf['human_clean_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bac632d-4898-4e95-8d88-2c4e6cece11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m classifier \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# text = \"the left gets absolutely roasted by the super awesome gigachad trump\" lable 1, score 0.938\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# text = \"the fed cuts interest rates\"  label 0, score 0.95\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m result \u001b[38;5;241m=\u001b[39m classifier(\u001b[43mtext\u001b[49m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# print(result)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# text = \"the left gets absolutely roasted by the super awesome gigachad trump\" lable 1, score 0.938\n",
    "# text = \"the fed cuts interest rates\"  label 0, score 0.95\n",
    "result = classifier(text)\n",
    "\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b7f7171-fd2b-4c6b-891f-de945e225a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_article(text):\n",
    "    # split into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Step 2: Run classifier on each sentence\n",
    "    try:\n",
    "        results = classifier(sentences)\n",
    "    \n",
    "    \n",
    "        # Step 3: Convert to array of scores (shape: [num_sentences, num_classes])\n",
    "        if not isinstance(results, list):\n",
    "            results = [results]\n",
    "    \n",
    "        score_array = np.array([r.get('score') for r in results])\n",
    "        mean_scores = score_array.mean(axis=0)\n",
    "    \n",
    "        # Step 4: Average across sentences\n",
    "        \n",
    "    \n",
    "        # Step 5: Final prediction\n",
    "        adjusted_score = 0\n",
    "        for r in results:\n",
    "            if r.get('label', None) == 'LABEL_1':\n",
    "                adjusted_score += r.get(\"score\", 0)\n",
    "            else:\n",
    "                adjusted_score -= r.get(\"score\", 0)\n",
    "        # pred_index = int(np.argmax(mean_scores))\n",
    "        # pred_label = results[0][pred_index]['label']\n",
    "        pred_label = \"LABEL_1\" if adjusted_score > 0 else \"LABEL_0\"\n",
    "    \n",
    "        return {\n",
    "            'label': pred_label,\n",
    "            'score': mean_scores,\n",
    "            'scores': score_array,\n",
    "            'adjusted_score': adjusted_score\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(results)\n",
    "        print(e)\n",
    "        return {\n",
    "            'label': \"error\",\n",
    "            'score': str(e),\n",
    "            'scores': results\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6f16f23-2da0-47ff-a0a7-391e1ec965a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "# wdf = wdf.sample(2)\n",
    "wdf['spinde_output'] = wdf['clean'].apply(classify_article)\n",
    "\n",
    "wdf['spinde_label'] = wdf['spinde_output'].apply(lambda x: x['label'])\n",
    "wdf['spinde_score'] = wdf['spinde_output'].apply(lambda x: x['score'])\n",
    "wdf['spinde_scores'] = wdf['spinde_output'].apply(lambda x: x['scores'])\n",
    "\n",
    "\n",
    "# [{'label': 'LABEL_0', 'score': 0.9084169268608093}, {'label': 'LABEL_0', 'score': 0.9547224640846252}]\n",
    "# string indices must be integers\n",
    "# [{'label': 'LABEL_0', 'score': 0.9597590565681458}]\n",
    "\n",
    "# string indices must be integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d683c976-a5c0-4d04-aee8-374190b57f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wdf = wdf.reset_index()\n",
    "# res = wdf.loc[0, 'spinde_output']\n",
    "\n",
    "wdf.head()\n",
    "wdf.to_csv('spinde_wide_form_jul30.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a8b9d9e-e40f-400c-ac36-3bcf1ffc180a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spinde_label\n",
       "LABEL_0    338\n",
       "LABEL_1     91\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdf['spinde_label'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
