{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5fae0ecd-19c4-4abc-ab12-5cc00f21c80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import evaluate\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "from lightning.pytorch.utilities.combined_loader import CombinedLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0cfc50f8-d49b-4dc6-bf1b-50fd7bf5e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf = pd.read_csv(\"../week5/week5_qbias_dataset.csv\")\n",
    "ldf.dropna(subset=['raw'], inplace=True)\n",
    "ldf = ldf[ldf['raw'].str.len() > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0462bbfc-2b9d-4e5f-aa1c-161d5ca5f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "TOKENIZERS_PARALLELISM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c0c9ce8c-539a-4e10-8f69-e9e8b647e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sk_multiclass_dataset(Dataset):\n",
    "    def __init__(self, values, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(\n",
    "            values.tolist(),\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        if labels.dtype == object or not np.issubdtype(labels.dtype, np.integer):\n",
    "            label_encoder = LabelEncoder()\n",
    "            torch_lables = torch.tensor(self.label_encoder.fit_transform(labels)).long()\n",
    "            self.label_mapping = {idx: label for idx, label in enumerate(label_encoder.classes_)}\n",
    "            self.num_classes = len(label_mapping)\n",
    "        else:\n",
    "            label_encoder = None\n",
    "            torch_lables = torch.tensor(labels.values).long()\n",
    "            self.label_mapping = None\n",
    "            self.num_classes = len(torch.unique(torch_lables))\n",
    "\n",
    "        self.X = values\n",
    "        self.y = torch_lables\n",
    "        assert self.X.shape[0] == self.y.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):    \n",
    "        # X = torch.from_numpy(self.X[idx].astype(np.int8).todense()).float().squeeze()\n",
    "        # y = self.y[idx]\n",
    "        # return X, y\n",
    "\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.y[idx]\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2078b4a6-a1d4-4ac3-958d-4176ffb6ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf = ldf.sample(4)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", device_map=\"auto\")\n",
    "\n",
    "train_ldf, test_ldf = train_test_split(ldf, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "t_l_ldf = sk_multiclass_dataset(train_ldf['raw'], train_ldf['label_left'], tokenizer)\n",
    "t_r_ldf = sk_multiclass_dataset(train_ldf['raw'], train_ldf['label_right'], tokenizer)\n",
    "t_c_ldf = sk_multiclass_dataset(train_ldf['raw'], train_ldf['label_center'], tokenizer)\n",
    "\n",
    "# need to have:\n",
    "trains = {\n",
    "    \"l\": DataLoader(t_l_ldf, batch_size=4, shuffle=True),\n",
    "    \"r\": DataLoader(t_r_ldf, batch_size=4, shuffle=True),\n",
    "    \"c\": DataLoader(t_c_ldf, batch_size=4, shuffle=True)\n",
    "}\n",
    "\n",
    "task_keys = list(trains.keys())\n",
    "\n",
    "combined_loader = CombinedLoader(trains, 'sequential')\n",
    "# _ = iter(combined_loader)\n",
    "\n",
    "# for batch, batch_idx, dataloader_idx in combined_loader:\n",
    "#     print(f\"{batch}, {batch_idx=}, {dataloader_idx=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "507c60f6-81c8-42bc-9355-c90ccd8d8d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTask_Network(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 tasks,\n",
    "                 hidden_dim : int = 200):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        # self.output_dim_l = output_dim_l\n",
    "        # self.output_dim_c = output_dim_c\n",
    "        # self.output_dim_r = output_dim_r\n",
    "        self.tasks = tasks\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.llama = AutoModel.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "        self.hidden = nn.Linear(self.llama.config.hidden_size, self.hidden_dim)\n",
    "        self.final_r = nn.Linear(self.hidden_dim, self.tasks[1]['output_size'])\n",
    "        self.final_l = nn.Linear(self.hidden_dim, self.tasks[2]['output_size'])\n",
    "        self.final_c = nn.Linear(self.hidden_dim, self.tasks[0]['output_size'])\n",
    "        \n",
    "        # for task in self.tasks:\n",
    "        #     if tasks[1]['pretrained']:\n",
    "        #         self.final_r.load_state_dict(torch.load(classifier_weights[\"final_r\"]))\n",
    "        #     if task == \"l\" and tasks[task]['pretrained']:\n",
    "        #         self.final_l.load_state_dict(torch.load(classifier_weights[\"final_l\"]))\n",
    "        #     if task == \"c\" and tasks[task]['pretrained']:\n",
    "        #         self.final_c.load_state_dict(torch.load(classifier_weights[\"final_c\"]))\n",
    "\n",
    "\n",
    "        #freeze llama?\n",
    "        # for param in self.llama.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "    \n",
    "    def forward(self, x, task_name : str):\n",
    "\n",
    "        outputs = self.llama(\n",
    "            input_ids=x[\"input_ids\"],\n",
    "            attention_mask=x[\"attention_mask\"]\n",
    "        )\n",
    "        iid = x[\"input_ids\"]\n",
    "        iid2 = x[\"attention_mask\"]\n",
    "        \n",
    "\n",
    "        # pooled = outputs.last_hidden_state[:, 0]  #was giving the same tokenization everytime - ig llama doesnt use the cls token?\n",
    "\n",
    "        last_hidden = outputs.last_hidden_state  # (B, T, H)\n",
    "\n",
    "        mask = x[\"attention_mask\"].unsqueeze(-1)  # (B, T, 1)\n",
    "        pooled = (last_hidden * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "\n",
    "        # if (random.randint(0,10) < 5):\n",
    "        # decoded = tokenizer.batch_decode(x[\"input_ids\"])\n",
    "        # print(f\"{decoded} got {pooled}\") \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        x = self.hidden(pooled)\n",
    "        # print(f\"task name is {task_name}\")\n",
    "        # print(x)\n",
    "        \n",
    "        #sigmoid? his example uses this but not llama\n",
    "        # x = torch.sigmoid(x)  \n",
    "        #think relu is better, although why need activation\n",
    "\n",
    "        laye = \"none\"\n",
    "        if task_name == 'r':\n",
    "            x = self.final_r(x)\n",
    "            # x = torch.sigmoid(self.final_r(x))\n",
    "            laye = \"final_r\"\n",
    "        elif task_name == 'l':\n",
    "            x = self.final_l(x)\n",
    "            # x = torch.sigmoid(self.final_l(x))\n",
    "        elif task_name == 'c':\n",
    "            x = self.final_c(x)\n",
    "            # x = torch.sigmoid(self.final_c(x))\n",
    "        else:\n",
    "            assert False, 'Bad Task ID passed'\n",
    "\n",
    "\n",
    "        # print(f\"Mean: {x.mean().item():.4f}, Min: {x.min().item():.4f}, Max: {x.max().item():.4f}\")\n",
    "\n",
    "           # and got mask {mask}\n",
    "           #  \\n\n",
    "\n",
    "        # print(f\"\"\"\n",
    "        #     __________________\n",
    "        #     printing for {task_name} with input_ids {iid} and attn {iid2}\n",
    "        #     \\n\n",
    "        #     has last hidden as {last_hidden}\n",
    "        #     \\n\n",
    " \n",
    "        #     and got pooled as {pooled}\n",
    "        #     \\n\n",
    "        #     for a final of {x}\n",
    "        #     \\n \n",
    "        #     layer used was {laye}\n",
    "        #     _______________________\n",
    "        #     \"\"\")\n",
    "\n",
    "        \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "83c9ca78-d4f6-4f24-950b-70e8ab05486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_loss = nn.BCEWithLogitsLoss()\n",
    "multiclass_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "tasks = [\n",
    "    # {\"name\": \"c\", \"output_size\" : 2, \"loss_func\" : nn.CrossEntropyLoss(), \"classifier_weights_loc\" : None, \"pretrained\": False },\n",
    "    {\"name\": \"c\", \"output_size\" : 1, \"loss_func\" : nn.BCEWithLogitsLoss(), \"classifier_weights_loc\" : None, \"pretrained\": False },\n",
    "    {\"name\": \"r\", \"output_size\" : 1, \"loss_func\" : nn.BCEWithLogitsLoss(), \"classifier_weights_loc\" : None, \"pretrained\": False },\n",
    "    {\"name\": \"l\", \"output_size\" : 1, \"loss_func\" : nn.BCEWithLogitsLoss(), \"classifier_weights_loc\" : None, \"pretrained\": False }\n",
    "]\n",
    "\n",
    "model = MultiTask_Network(128, tasks)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1b962737-a826-4db8-bfba-3724175bb310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: -0.3905 | Label: [0.0]\n",
      "Pred: 0.1137 | Label: [1.0]\n",
      "Pred: -0.1498 | Label: [0.0]\n",
      "tensor(0.5919, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Pred: 0.1383 | Label: [0.0]\n",
      "Pred: 0.4855 | Label: [0.0]\n",
      "Pred: -0.0009 | Label: [0.0]\n",
      "tensor(0.8075, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Pred: 0.1384 | Label: [1.0]\n",
      "Pred: 0.1757 | Label: [0.0]\n",
      "Pred: 0.2807 | Label: [1.0]\n",
      "tensor(0.6579, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Epoch 1, Loss: 0.685784121354421\n",
      "Pred: -2.4067 | Label: [0.0]\n",
      "Pred: -2.6486 | Label: [0.0]\n",
      "Pred: 1.2392 | Label: [1.0]\n",
      "tensor(0.1363, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Pred: -1.9904 | Label: [0.0]\n",
      "Pred: -2.2620 | Label: [0.0]\n",
      "Pred: -3.0193 | Label: [0.0]\n",
      "tensor(0.0916, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Pred: 1.5636 | Label: [1.0]\n",
      "Pred: 1.6370 | Label: [1.0]\n",
      "Pred: -2.0091 | Label: [0.0]\n",
      "tensor(0.1646, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Epoch 2, Loss: 0.13083790491024652\n",
      "Pred: -3.0664 | Label: [0.0]\n",
      "Pred: -3.4043 | Label: [0.0]\n",
      "Pred: 2.5119 | Label: [1.0]\n",
      "tensor(0.0521, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Pred: -4.9419 | Label: [0.0]\n",
      "Pred: -3.9225 | Label: [0.0]\n",
      "Pred: -4.1769 | Label: [0.0]\n",
      "tensor(0.0140, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Pred: -3.8231 | Label: [0.0]\n",
      "Pred: 3.0958 | Label: [1.0]\n",
      "Pred: 3.3480 | Label: [1.0]\n",
      "tensor(0.0335, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Epoch 3, Loss: 0.03317637493213018\n",
      "Pred: 3.5994 | Label: [1.0]\n",
      "Pred: -4.1212 | Label: [0.0]\n",
      "Pred: -3.7568 | Label: [0.0]\n",
      "tensor(0.0221, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Pred: -6.0268 | Label: [0.0]\n",
      "Pred: -5.5085 | Label: [0.0]\n",
      "Pred: -5.6214 | Label: [0.0]\n",
      "tensor(0.0034, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Pred: 4.4600 | Label: [1.0]\n",
      "Pred: -5.0828 | Label: [0.0]\n",
      "Pred: 4.8879 | Label: [1.0]\n",
      "tensor(0.0084, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Epoch 4, Loss: 0.01126821874640882\n",
      "Pred: 4.7923 | Label: [1.0]\n",
      "Pred: -4.3279 | Label: [0.0]\n",
      "Pred: -4.7464 | Label: [0.0]\n",
      "tensor(0.0100, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Pred: -6.6714 | Label: [0.0]\n",
      "Pred: -6.4673 | Label: [0.0]\n",
      "Pred: -6.5561 | Label: [0.0]\n",
      "tensor(0.0014, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Pred: 6.0290 | Label: [1.0]\n",
      "Pred: -6.0346 | Label: [0.0]\n",
      "Pred: 5.5624 | Label: [1.0]\n",
      "tensor(0.0029, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Epoch 5, Loss: 0.004764413771529992\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for i in range(10): #epochs\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch, batch_idx, dataloader_idx in combined_loader:\n",
    "        # print(f\"{batch}, {batch_idx=}, {dataloader_idx=}\")\n",
    "        \n",
    "        preds = model(batch, task_name = tasks[dataloader_idx]['name'])\n",
    "        curr_loss_func = tasks[dataloader_idx]['loss_func']\n",
    "        loss = curr_loss_func(preds, batch['labels'].float().unsqueeze(1))\n",
    "        k = 0\n",
    "        for p, l in zip(preds.squeeze().tolist(), batch['labels'].float().unsqueeze(1).tolist()):\n",
    "            print(f\"Pred: {p:.4f} | Label: {l}\")\n",
    "            k+=1\n",
    "            if k > 20:\n",
    "                break\n",
    "        print (loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {i+1}, Loss: {total_loss/len(batch)}\")\n",
    "\n",
    "\n",
    "\n",
    "# save_dir = \"./multitask_01_sunday_8pm_Fexamples_multiEpoch\"\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# torch.save(model.state_dict(), os.path.join(save_dir, \"model.pth\"))\n",
    "\n",
    "# # Save tokenizer\n",
    "# tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "# print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d5a6f2e3-44b0-4750-ad61-387bdd711b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not model:\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = MultiTask_Network(128, tasks)\n",
    "save_dir = \"./multitask_01_sunday_8pm_Fexamples_multiEpoch\"\n",
    "model.load_state_dict(torch.load(os.path.join(save_dir, \"model.pth\"), map_location=device))\n",
    "test_ldf = ldf.sample(30).copy()\n",
    "# train_ldf, test_ldf = train_test_split(ldf, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "e_l_ldf = sk_multiclass_dataset(test_ldf['raw'], test_ldf['label_left'], tokenizer)\n",
    "e_r_ldf = sk_multiclass_dataset(test_ldf['raw'], test_ldf['label_right'], tokenizer)\n",
    "e_c_ldf = sk_multiclass_dataset(test_ldf['raw'], test_ldf['label_center'], tokenizer)\n",
    "\n",
    "# need to have:\n",
    "evals = {\n",
    "    \"l\": DataLoader(e_l_ldf, batch_size=4, shuffle=True),\n",
    "    \"r\": DataLoader(e_r_ldf, batch_size=4, shuffle=True),\n",
    "    \"c\": DataLoader(e_c_ldf, batch_size=4, shuffle=True)\n",
    "}\n",
    "\n",
    "task_keys = list(trains.keys())\n",
    "\n",
    "combined_eval_loader = CombinedLoader(evals, 'sequential')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "24d15996-a971-442c-b748-948328055adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "660"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ldf.iloc[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "12c3b669-801b-4d14-8d06-6d5e4a606739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performance\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80        60\n",
      "           1       0.00      0.00      0.00        30\n",
      "\n",
      "    accuracy                           0.67        90\n",
      "   macro avg       0.33      0.50      0.40        90\n",
      "weighted avg       0.44      0.67      0.53        90\n",
      "\n",
      "test performance on task c\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89        24\n",
      "           1       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.80        30\n",
      "   macro avg       0.40      0.50      0.44        30\n",
      "weighted avg       0.64      0.80      0.71        30\n",
      "\n",
      "test performance on task r\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80        20\n",
      "           1       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.67        30\n",
      "   macro avg       0.33      0.50      0.40        30\n",
      "weighted avg       0.44      0.67      0.53        30\n",
      "\n",
      "test performance on task l\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      1.00      0.70        16\n",
      "           1       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.53        30\n",
      "   macro avg       0.27      0.50      0.35        30\n",
      "weighted avg       0.28      0.53      0.37        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ihome/xli/sek188/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/ihome/xli/sek188/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/ihome/xli/sek188/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/ihome/xli/sek188/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/ihome/xli/sek188/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/ihome/xli/sek188/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/ihome/xli/sek188/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/ihome/xli/sek188/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/ihome/xli/sek188/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/ihome/xli/sek188/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/ihome/xli/sek188/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/ihome/xli/sek188/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "for batch, batch_idx, dataloader_idx in combined_eval_loader:\n",
    "\n",
    "    task_name = tasks[dataloader_idx]['name']\n",
    "    preds = model(batch, task_name)\n",
    "    preds_np = preds.detach().cpu().numpy().flatten()\n",
    "    ypreds = torch.sigmoid(torch.tensor(preds_np)).numpy()\n",
    "    \n",
    "    labels_np = batch['labels'].detach().cpu().numpy().flatten()\n",
    "\n",
    "    # pred_classes = (preds_np > 0.5).astype(int)\n",
    "    probs = torch.sigmoid(torch.tensor(preds_np))\n",
    "    pred_classes = (probs > 0.5).int().numpy()\n",
    "\n",
    "    for y_p, y_t, y_c in zip(preds_np, labels_np, pred_classes):\n",
    "        results.append({\n",
    "            \"task\": task_name,\n",
    "            \"y_pred\": float(y_p),\n",
    "            \"y_true\": int(y_t),\n",
    "            \"pred_class\": int(y_c)\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"test performance\")\n",
    "print(classification_report(df_results['y_true'], df_results['pred_class']))\n",
    "\n",
    "for t in range(3):\n",
    "    task_name = task_keys[t]\n",
    "    subdf = df_results[df_results['task'] == task_name].copy()\n",
    "    subdf.head()\n",
    "    print(f\"test performance on task {tasks[t]['name']}\")\n",
    "    print(classification_report(subdf['y_true'], subdf['pred_class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e4dc3607-d518-43a8-bef0-6980a7c749b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y_pred\n",
       "-1.579535    1\n",
       "-2.522428    1\n",
       "-2.382221    1\n",
       "-1.403132    1\n",
       "-2.432225    1\n",
       "            ..\n",
       "-1.794623    1\n",
       "-1.904278    1\n",
       "-2.264257    1\n",
       "-1.843960    1\n",
       "-1.675348    1\n",
       "Name: count, Length: 160, dtype: int64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf = df_results[df_results['task'] == 'l'].copy()\n",
    "subdf['y_pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a52086a7-7cf7-43ff-8524-c3115b42fccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden.weight grad norm: 0.49672582745552063\n",
      "hidden.bias grad norm: 0.014233889058232307\n",
      "final_l.weight grad norm: 12.833967208862305\n",
      "final_l.bias grad norm: 0.30220913887023926\n"
     ]
    }
   ],
   "source": [
    "# print(model.final_l.weight)\n",
    "# preds\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # print(name)\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name} grad norm: {param.grad.norm().item()}\")\n",
    "\n",
    "# model.forward(e_l_ldf[0], 'l')\n",
    "# outputs = model.llama(\n",
    "#             input_ids=e_l_ldf[0][\"input_ids\"],\n",
    "#             attention_mask=e_l_ldf[0][\"attention_mask\"]\n",
    "#         )\n",
    "# outputs.last_hidden_state[:, 0].std(dim=0)\n",
    "# # e_l_ldf[0][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2e2b1d78-4734-46d8-8c58-eb37b7116808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "task\n",
       "c    160\n",
       "r    160\n",
       "l    160\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results['task'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
