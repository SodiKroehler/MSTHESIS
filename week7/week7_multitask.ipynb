{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5fae0ecd-19c4-4abc-ab12-5cc00f21c80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import evaluate\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "from lightning.pytorch.utilities.combined_loader import CombinedLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0cfc50f8-d49b-4dc6-bf1b-50fd7bf5e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf = pd.read_csv(\"../week5/week5_qbias_dataset.csv\")\n",
    "ldf.dropna(subset=['raw'], inplace=True)\n",
    "ldf = ldf[ldf['raw'].str.len() > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0462bbfc-2b9d-4e5f-aa1c-161d5ca5f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "TOKENIZERS_PARALLELISM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c0c9ce8c-539a-4e10-8f69-e9e8b647e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sk_multiclass_dataset(Dataset):\n",
    "    def __init__(self, values, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(\n",
    "            values.tolist(),\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        if labels.dtype == object or not np.issubdtype(labels.dtype, np.integer):\n",
    "            label_encoder = LabelEncoder()\n",
    "            torch_lables = torch.tensor(self.label_encoder.fit_transform(labels)).long()\n",
    "            self.label_mapping = {idx: label for idx, label in enumerate(label_encoder.classes_)}\n",
    "            self.num_classes = len(label_mapping)\n",
    "        else:\n",
    "            label_encoder = None\n",
    "            torch_lables = torch.tensor(labels.values).long()\n",
    "            self.label_mapping = None\n",
    "            self.num_classes = len(torch.unique(torch_lables))\n",
    "\n",
    "        self.X = values\n",
    "        self.y = torch_lables\n",
    "        assert self.X.shape[0] == self.y.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):    \n",
    "        # X = torch.from_numpy(self.X[idx].astype(np.int8).todense()).float().squeeze()\n",
    "        # y = self.y[idx]\n",
    "        # return X, y\n",
    "\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.y[idx]\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2078b4a6-a1d4-4ac3-958d-4176ffb6ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ldf = ldf.sample(2000)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", device_map=\"auto\")\n",
    "\n",
    "train_ldf, test_ldf = train_test_split(ldf, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "t_l_ldf = sk_multiclass_dataset(train_ldf['raw'], train_ldf['label_left'], tokenizer)\n",
    "t_r_ldf = sk_multiclass_dataset(train_ldf['raw'], train_ldf['label_right'], tokenizer)\n",
    "t_c_ldf = sk_multiclass_dataset(train_ldf['raw'], train_ldf['label_center'], tokenizer)\n",
    "\n",
    "# need to have:\n",
    "trains = {\n",
    "    \"l\": DataLoader(t_l_ldf, batch_size=4, shuffle=True),\n",
    "    \"r\": DataLoader(t_r_ldf, batch_size=4, shuffle=True),\n",
    "    \"c\": DataLoader(t_c_ldf, batch_size=4, shuffle=True)\n",
    "}\n",
    "\n",
    "task_keys = list(trains.keys())\n",
    "\n",
    "combined_loader = CombinedLoader(trains, 'sequential')\n",
    "# _ = iter(combined_loader)\n",
    "\n",
    "# for batch, batch_idx, dataloader_idx in combined_loader:\n",
    "#     print(f\"{batch}, {batch_idx=}, {dataloader_idx=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "507c60f6-81c8-42bc-9355-c90ccd8d8d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTask_Network(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 tasks,\n",
    "                 hidden_dim : int = 200):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        # self.output_dim_l = output_dim_l\n",
    "        # self.output_dim_c = output_dim_c\n",
    "        # self.output_dim_r = output_dim_r\n",
    "        self.tasks = tasks\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.llama = AutoModel.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "        self.hidden = nn.Linear(self.llama.config.hidden_size, self.hidden_dim)\n",
    "        self.final_r = nn.Linear(self.hidden_dim, self.tasks[1]['output_size'])\n",
    "        self.final_l = nn.Linear(self.hidden_dim, self.tasks[2]['output_size'])\n",
    "        self.final_c = nn.Linear(self.hidden_dim, self.tasks[0]['output_size'])\n",
    "        \n",
    "        # for task in self.tasks:\n",
    "        #     if tasks[1]['pretrained']:\n",
    "        #         self.final_r.load_state_dict(torch.load(classifier_weights[\"final_r\"]))\n",
    "        #     if task == \"l\" and tasks[task]['pretrained']:\n",
    "        #         self.final_l.load_state_dict(torch.load(classifier_weights[\"final_l\"]))\n",
    "        #     if task == \"c\" and tasks[task]['pretrained']:\n",
    "        #         self.final_c.load_state_dict(torch.load(classifier_weights[\"final_c\"]))\n",
    "\n",
    "\n",
    "        #freeze llama?\n",
    "        for param in self.llama.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    \n",
    "    def forward(self, x, task_name : str):\n",
    "\n",
    "        outputs = self.llama(\n",
    "            input_ids=x[\"input_ids\"],\n",
    "            attention_mask=x[\"attention_mask\"]\n",
    "        )\n",
    "        \n",
    "\n",
    "        # pooled = outputs.last_hidden_state[:, 0]  #was giving the same tokenization everytime - ig llama doesnt use the cls token?\n",
    "\n",
    "        last_hidden = outputs.last_hidden_state  # (B, T, H)\n",
    "        mask = x[\"attention_mask\"].unsqueeze(-1)  # (B, T, 1)\n",
    "        pooled = (last_hidden * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "\n",
    "        # if (random.randint(0,10) < 5):\n",
    "        # decoded = tokenizer.batch_decode(x[\"input_ids\"])\n",
    "        # print(f\"{decoded} got {pooled}\") \n",
    "\n",
    "        \n",
    "        x = self.hidden(pooled)\n",
    "        \n",
    "        #sigmoid? his example uses this but not llama\n",
    "        # x = torch.sigmoid(x)  \n",
    "        #think relu is better, although why need activation\n",
    "\n",
    "        if task_name == 'r':\n",
    "            x = self.final_r(x)\n",
    "        elif task_name == 'l':\n",
    "            x = self.final_l(x)\n",
    "        elif task_name == 'c':\n",
    "            x = self.final_c(x)\n",
    "        else:\n",
    "            assert False, 'Bad Task ID passed'\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "1b962737-a826-4db8-bfba-3724175bb310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 302.0980979005496\n",
      "Epoch 2, Loss: 249.62065717577934\n",
      "Epoch 3, Loss: 254.29995453854403\n",
      "Epoch 4, Loss: 256.56996925671893\n",
      "Epoch 5, Loss: 253.9098894794782\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "binary_loss = nn.BCEWithLogitsLoss()\n",
    "multiclass_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "tasks = [\n",
    "    # {\"name\": \"c\", \"output_size\" : 2, \"loss_func\" : nn.CrossEntropyLoss(), \"classifier_weights_loc\" : None, \"pretrained\": False },\n",
    "    {\"name\": \"c\", \"output_size\" : 1, \"loss_func\" : nn.BCEWithLogitsLoss(), \"classifier_weights_loc\" : None, \"pretrained\": False },\n",
    "    {\"name\": \"r\", \"output_size\" : 1, \"loss_func\" : nn.BCEWithLogitsLoss(), \"classifier_weights_loc\" : None, \"pretrained\": False },\n",
    "    {\"name\": \"l\", \"output_size\" : 1, \"loss_func\" : nn.BCEWithLogitsLoss(), \"classifier_weights_loc\" : None, \"pretrained\": False }\n",
    "]\n",
    "\n",
    "model = MultiTask_Network(128, tasks)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "\n",
    "\n",
    "for i in range(5): #epochs\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch, batch_idx, dataloader_idx in combined_loader:\n",
    "        # print(f\"{batch}, {batch_idx=}, {dataloader_idx=}\")\n",
    "        \n",
    "        preds = model(batchX, task_name = tasks[dataloader_idx]['name'])\n",
    "        curr_loss_func = tasks[dataloader_idx]['loss_func']\n",
    "        loss = curr_loss_func(preds, batch['labels'].float().unsqueeze(1))\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {i+1}, Loss: {total_loss/len(batch)}\")\n",
    "\n",
    "\n",
    "\n",
    "save_dir = \"./multitask_01_sunday_8pm_Fexamples_multiEpoch\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(save_dir, \"model.pth\"))\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d5a6f2e3-44b0-4750-ad61-387bdd711b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not model:\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = MultiTask_Network(128, tasks)\n",
    "save_dir = \"./multitask_01_sunday_8pm_Fexamples_multiEpoch\"\n",
    "model.load_state_dict(torch.load(os.path.join(save_dir, \"model.pth\"), map_location=device))\n",
    "# ldf = ldf.sample(3000)\n",
    "train_ldf, test_ldf = train_test_split(ldf, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "e_l_ldf = sk_multiclass_dataset(test_ldf['raw'], test_ldf['label_left'], tokenizer)\n",
    "e_r_ldf = sk_multiclass_dataset(test_ldf['raw'], test_ldf['label_right'], tokenizer)\n",
    "e_c_ldf = sk_multiclass_dataset(test_ldf['raw'], test_ldf['label_center'], tokenizer)\n",
    "\n",
    "# need to have:\n",
    "evals = {\n",
    "    \"l\": DataLoader(e_l_ldf, batch_size=4, shuffle=True),\n",
    "    \"r\": DataLoader(e_r_ldf, batch_size=4, shuffle=True),\n",
    "    \"c\": DataLoader(e_c_ldf, batch_size=4, shuffle=True)\n",
    "}\n",
    "\n",
    "task_keys = list(trains.keys())\n",
    "\n",
    "combined_eval_loader = CombinedLoader(evals, 'sequential')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3b669-801b-4d14-8d06-6d5e4a606739",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "for batch, batch_idx, dataloader_idx in combined_eval_loader:\n",
    "\n",
    "    task_name = tasks[dataloader_idx]['name']\n",
    "    preds = model(batch, task_name)\n",
    "    preds_np = preds.detach().cpu().numpy().flatten()\n",
    "    ypreds = torch.sigmoid(torch.tensor(preds_np)).numpy()\n",
    "    \n",
    "    labels_np = batch['labels'].detach().cpu().numpy().flatten()\n",
    "\n",
    "    pred_classes = (preds_np > 0.5).astype(int)\n",
    "\n",
    "    for y_p, y_t, y_c in zip(preds_np, labels_np, pred_classes):\n",
    "        results.append({\n",
    "            \"task\": task_name,\n",
    "            \"y_pred\": float(y_p),\n",
    "            \"y_true\": int(y_t),\n",
    "            \"pred_class\": int(y_c)\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"test performance\")\n",
    "print(classification_report(df_results['y_true'], df_results['pred_class']))\n",
    "\n",
    "for t in range(3):\n",
    "    task_name = task_keys[t]\n",
    "    subdf = df_results[df_results['task'] == task_name].copy()\n",
    "    subdf.head()\n",
    "    print(f\"test performance on task {tasks[t]['name']}\")\n",
    "    print(classification_report(subdf['y_true'], subdf['pred_class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e4dc3607-d518-43a8-bef0-6980a7c749b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y_pred\n",
       "-1.579535    1\n",
       "-2.522428    1\n",
       "-2.382221    1\n",
       "-1.403132    1\n",
       "-2.432225    1\n",
       "            ..\n",
       "-1.794623    1\n",
       "-1.904278    1\n",
       "-2.264257    1\n",
       "-1.843960    1\n",
       "-1.675348    1\n",
       "Name: count, Length: 160, dtype: int64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf = df_results[df_results['task'] == 'l'].copy()\n",
    "subdf['y_pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a52086a7-7cf7-43ff-8524-c3115b42fccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden.weight grad norm: 0.49672582745552063\n",
      "hidden.bias grad norm: 0.014233889058232307\n",
      "final_l.weight grad norm: 12.833967208862305\n",
      "final_l.bias grad norm: 0.30220913887023926\n"
     ]
    }
   ],
   "source": [
    "# print(model.final_l.weight)\n",
    "# preds\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # print(name)\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name} grad norm: {param.grad.norm().item()}\")\n",
    "\n",
    "# model.forward(e_l_ldf[0], 'l')\n",
    "# outputs = model.llama(\n",
    "#             input_ids=e_l_ldf[0][\"input_ids\"],\n",
    "#             attention_mask=e_l_ldf[0][\"attention_mask\"]\n",
    "#         )\n",
    "# outputs.last_hidden_state[:, 0].std(dim=0)\n",
    "# # e_l_ldf[0][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2e2b1d78-4734-46d8-8c58-eb37b7116808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "task\n",
       "c    160\n",
       "r    160\n",
       "l    160\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results['task'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
