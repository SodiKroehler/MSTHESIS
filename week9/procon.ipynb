{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2434945-a210-434b-8983-a272a5ba1d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from rapidfuzz import fuzz, process\n",
    "from tqdm import tqdm\n",
    "# from sodigpt import sodiGPT\n",
    "# import sodigpt\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from rapidfuzz import fuzz, process\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7cf0b5-dcfe-49ed-93dc-ffebada6ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !pip install rapidfuzz\n",
    "!pip install tiktoken\n",
    "# importlib.reload(sodigpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02cd6c77-397d-4ee5-8c52-54f8ca3304d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('procon20/data/ProconDual/dev.tsv', sep='\\t', names=['respType', 'q', 'a'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1a8b55dc-d894-4f4f-abe1-77a62e1f4366",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class sodiGPT:\n",
    "    def __init__(self, system_prompt = \"You are a helpful assistant.\"):\n",
    "        OPENAI_API_KEY = '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'\n",
    "        self.client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        self.system_prompt = system_prompt\n",
    "        self.temperature = 0\n",
    "        self.batch_size=5\n",
    "        self.max_tokens=1000\n",
    "        self.temperature=0\n",
    "        self.prompt=\"You are a helpful assistant.\"\n",
    "        self.pattern=\"\"\n",
    "        self.source_column_name=\"text1\"\n",
    "        self.target_column_llmresp=\"llm_output\"\n",
    "        self.target_columns = [\"out\"]\n",
    "        self.target_columns_in_json = [\"out\"]\n",
    "        self.output_json_parser=json.loads\n",
    "        self.json_delimiter=\"```\"\n",
    "\n",
    "        #rate limit\n",
    "        self.tpm_limit = 30000\n",
    "        self.used_tokens = 0\n",
    "        self.window_start = time.time()\n",
    "        self.max_retries = 5\n",
    "\n",
    "        \n",
    "    def reset_window(self):\n",
    "        self.window_start = time.time()\n",
    "        self.used_tokens = 0\n",
    "\n",
    "    def wait_if_needed(self, new_tokens):\n",
    "        if new_tokens > self.tpm_limit:\n",
    "            raise ValueError(f\"Request for {new_tokens} tokens exceeds the tpm_limit of {self.tpm_limit} — cannot proceed.\")\n",
    "    \n",
    "        retries = 0\n",
    "        while retries <= self.max_retries:\n",
    "            now = time.time()\n",
    "            elapsed = now - self.window_start\n",
    "    \n",
    "            if elapsed > 60:\n",
    "                self.reset_window()\n",
    "    \n",
    "            if self.used_tokens + new_tokens <= self.tpm_limit:\n",
    "                self.used_tokens += new_tokens\n",
    "                return  # success!\n",
    "    \n",
    "            # Throttle if needed\n",
    "            sleep_time = 60 - elapsed\n",
    "            print(f\"⏳ Throttling... sleeping for {sleep_time:.2f}s (retry {retries + 1}/{self.max_retries})\")\n",
    "            time.sleep(sleep_time)\n",
    "            self.reset_window()\n",
    "            retries += 1\n",
    "    \n",
    "        raise RuntimeError(f\"Token limit could not be satisfied after {self.max_retries} retries.\")\n",
    "\n",
    "\n",
    "    def count_tokens(self, text, model=\"gpt-4o\"):\n",
    "        enc = tiktoken.encoding_for_model(model)\n",
    "        # encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        return len(enc.encode(text))\n",
    "\n",
    "    def call_getResponse(self, prompt, pattern=\"\", text=\"\"):\n",
    "        if pattern and pattern != \"\":\n",
    "            prompt = prompt.replace(pattern, text)\n",
    "\n",
    "        try:\n",
    "            # print(f\"\\n\\n{self.system_prompt} \\n\\n {self.prompt}\")\n",
    "\n",
    "            # print(\"TOKEN COUNT:\", self.count_tokens(f\"\\n\\n{self.system_prompt} \\n\\n {self.prompt}\"))\n",
    "\n",
    "            req_token_count = prompt_tokens = self.count_tokens(f\"\\n\\n{self.system_prompt} \\n\\n {prompt}\")\n",
    "            if req_token_count > self.tpm_limit:\n",
    "                raise ValueError(f\"Prompt token count exceeds limit: {req_token_count} > {self.tpm_limit}\") \n",
    "            self.wait_if_needed(2 * prompt_tokens)\n",
    "    \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=1000,\n",
    "                temperature=self.temperature\n",
    "            )\n",
    "\n",
    "            # output = response['choices'][0]['message']['content'].strip()\n",
    "            # output = response.choices[0].message.content.strip()\n",
    "\n",
    "            # response = input()\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def call(self, prompt, pattern=\"\", text=\"\"):\n",
    "        resp = self.call_getResponse(prompt, pattern, text)\n",
    "        if resp is None:\n",
    "            return None\n",
    "        else:\n",
    "            try:\n",
    "                output = resp.choices[0].message.content.strip()\n",
    "                return output\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                return None\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def get_json_string_from_llmresp(self, llm_response, delimiter=None):\n",
    "        if not delimiter:\n",
    "            delimiter = self.json_delimiter\n",
    "\n",
    "        if not llm_response:\n",
    "            raise ValueError(\"No llm response in json parser\")  \n",
    "            \n",
    "        start, end = 0, len(llm_response)\n",
    "\n",
    "        delim_match = list(re.finditer(delimiter, llm_response))\n",
    "        if len(delim_match) >= 2:\n",
    "            start = delim_match[0].start()\n",
    "            end = delim_match[-1].end() - len(delimiter)\n",
    "\n",
    "            # Check for the word \"json\" right after the first delimiter and remove it if it exists\n",
    "            after_first_delim = llm_response[start + len(delimiter):].strip()\n",
    "            if after_first_delim.lower().startswith(\"json\"):\n",
    "                start += len(delimiter) + len(\"json\")\n",
    "            else:\n",
    "                start += len(delimiter)\n",
    "        else:\n",
    "            print(f\"Not enough occurrences of delimiter '{delimiter}' in the response.\")\n",
    "\n",
    "        return llm_response[start:end].strip()\n",
    "\n",
    "        \n",
    "    def get_int_from_llmresp(self, llm_response, default_value=-1.0):\n",
    "        try:\n",
    "            lint = re.search(r'\\d+\\.\\d+', llm_response)\n",
    "            return float(lint.group())\n",
    "        except Exception as e:\n",
    "            print(f\"Error in parsing float from response: {e}\")\n",
    "            return default_value\n",
    "        \n",
    "    def get_int_from_llmresp(self, llm_response, default_value=-1):\n",
    "        try:\n",
    "            lint = re.search(r'\\d+', llm_response)\n",
    "            return int(lint.group())\n",
    "        except Exception as e:\n",
    "            print(f\"Error in parsing integer from response: {e}\")\n",
    "            return default_value\n",
    "        \n",
    "\n",
    "    def parse_json(self, llm_response):\n",
    "        try:\n",
    "            lint = json.loads(llm_response)\n",
    "            return lint\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON parsing error at line {e.lineno}, column {e.colno}: {e.msg}\")\n",
    "            print(f\"Failed JSON snippet: {llm_response[e.pos-50:e.pos+50]}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error in parsing JSON: {e}\")\n",
    "            return None\n",
    "\n",
    "    \n",
    "    def fuzz_get(self, obj, search_key, desired_type, default_value=None):\n",
    "        score_cuttoff = 80\n",
    "        if default_value is None:\n",
    "            default_value = desired_type()\n",
    "\n",
    "        if not obj:\n",
    "            return default_value\n",
    "        \n",
    "        if not search_key and isinstance(obj, desired_type):\n",
    "            return obj\n",
    "        elif not search_key:\n",
    "            return default_value\n",
    "        \n",
    "        if isinstance(obj, dict):\n",
    "            # print(f\"in fuzzget, looking for string {search_key} in object {obj}\")\n",
    "            if search_key in obj and isinstance(obj[search_key], desired_type):\n",
    "                return obj[search_key]\n",
    "            else:\n",
    "                possMatches = []\n",
    "                for k, v in obj.items():\n",
    "                    score = fuzz.ratio(search_key, k)\n",
    "                    if score > score_cuttoff and isinstance(v, desired_type):\n",
    "                        return v\n",
    "                    elif score > score_cuttoff:\n",
    "                        possMatches.append([k, v])\n",
    "                if len(possMatches) < 1:\n",
    "                    return default_value\n",
    "                else:\n",
    "                    possMatches = sorted(possMatches, key=lambda x: x[1], reverse=True)\n",
    "                    return possMatches[0][1]\n",
    "        elif isinstance(obj, desired_type):\n",
    "            return obj\n",
    "        else:\n",
    "            return default_value\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def call_batch(self, batch):\n",
    "        if batch.shape[0] < 1:\n",
    "            print(\"empty batch\")\n",
    "            return None\n",
    "\n",
    "        # Ensure index is 0-based and add an explicit index column\n",
    "        batch = batch.reset_index(drop=True)\n",
    "        batch[\"row_index\"] = batch.index\n",
    "\n",
    "\n",
    "        joined_texts = \"\\n\".join([\n",
    "            f\"{i}. {text}\" for i, text in zip(batch[\"row_index\"], batch[self.source_column_name])\n",
    "        ])\n",
    "        \n",
    "        preg_prompt = self.prompt.replace(self.pattern, joined_texts)\n",
    "        # print(f\" pattern is {self.pattern} and prompt is {prompt}\")\n",
    "\n",
    "        try:\n",
    "\n",
    "            response = self.call(preg_prompt)\n",
    "            output = self.parse_json(self.get_json_string_from_llmresp(response))\n",
    "\n",
    "            if output is None:\n",
    "                print(\"parsing failed\")\n",
    "                batch.loc[0, self.target_column_llmresp] = response\n",
    "                batch.drop(columns=[\"row_index\"], inplace=True)\n",
    "                return batch\n",
    "            \n",
    "            output_by_index = {}\n",
    "            for item in output:\n",
    "                theID = self.fuzz_get(item, \"id\", str, None)\n",
    "                if theID is not None:\n",
    "                    if theID not in output_by_index:\n",
    "                        output_by_index[theID] = {}\n",
    "                    for jtarget in self.target_columns_in_json:\n",
    "                        output_by_index[theID][jtarget] = self.fuzz_get(item, jtarget, str, \"none found\")\n",
    "\n",
    "            # print(output_by_index)\n",
    "\n",
    "            for target_col, json_key in zip(self.target_columns, self.target_columns_in_json):\n",
    "                batch[target_col] = batch[\"row_index\"].apply(\n",
    "                    lambda idx: output_by_index.get(idx).get(json_key)\n",
    "                )\n",
    "            # Remove the explicit index column\n",
    "            batch.drop(columns=[\"row_index\"], inplace=True)\n",
    "            return batch\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch processing: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "        \n",
    "    def process_df_chunked(self, df):\n",
    "        if df.shape[0] < 1:\n",
    "            print(\"empty batch\")\n",
    "            return None\n",
    "        \n",
    "        start = time.time()\n",
    "        num_batchs = int(np.ceil(df.shape[0] / self.batch_size))\n",
    "\n",
    "        self.reset_window()\n",
    "        for i in tqdm(range(num_batchs), desc=\"Processing Batches\", unit=\"batch\"):\n",
    "            start_idx = i * self.batch_size\n",
    "            end_idx = min((i + 1) * self.batch_size, df.shape[0])\n",
    "            batch = df.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "            print(f\"starting batch between idx {start_idx} and {end_idx}\")\n",
    "\n",
    "            batch = self.call_batch(batch)\n",
    "            if batch is None:\n",
    "                print(f\"Batch {i} processing failed\")\n",
    "                continue\n",
    "\n",
    "            # df.iloc[start_idx:end_idx] = batch\n",
    "            # for col in batch.columns:\n",
    "            #     df.loc[start_idx:end_idx, col] = batch[col].values\n",
    "\n",
    "            for col in batch.columns:\n",
    "                if col not in df.columns:\n",
    "                    default_value = batch[col].dtype.type() if pd.api.types.is_numeric_dtype(batch[col]) else ''\n",
    "                    df[col] = default_value\n",
    "\n",
    "                # print(batch[col])\n",
    "                df.iloc[start_idx:end_idx, df.columns.get_loc(col)] = batch[col].values\n",
    "\n",
    "        end = time.time()\n",
    "        duration = (end - start) * 1000\n",
    "\n",
    "        return df, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d3c157-4e64-4d9e-9e19-c7a600b0b968",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = {\n",
    "    1: \"Political Factors and Implications\",\n",
    "    2: \"Public Sentiment\",\n",
    "    3: \"Cultural Identity\",\n",
    "    4: \"Morality and Ethics\",\n",
    "    5: \"Fairness and Equality\",\n",
    "    6: \"Legality, Constitutionality, Jurisdiction\",\n",
    "    7: \"Crime and Punishment\",\n",
    "    8: \"Security and Defense\",\n",
    "    9: \"Health and Safety\",\n",
    "    10: \"Quality of Life\",\n",
    "    11: \"Economics\",\n",
    "    12: \"Capacity and Resources\",\n",
    "    13: \"Policy Description, Prescription, Evaluation\",\n",
    "    14: \"External Regulation and Reputation\",\n",
    "    15: \"Other\"\n",
    "}\n",
    "\n",
    "y_labels__string = json.dumps(y_labels, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2cc726-e07e-438f-afd5-8dd008bad9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.iloc[4,2]\n",
    "sdf = df[df['q'] == df.loc[4, 'q']]\n",
    "sdf.to_csv('sample_procon.csv')\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b6778d8f-322c-483d-8327-291d0290b905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/slurm-1118839/ipykernel_6809/3922876181.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_by_frame = harvard_df.groupby('frame').apply(lambda x: x.sample(n=2, random_state=42)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "harvard_df = pd.read_csv('final_pca_and_gpt_framing.csv')\n",
    "grouped_by_frame = harvard_df.groupby('frame').apply(lambda x: x.sample(n=2, random_state=42)).reset_index(drop=True)\n",
    "grouped_by_frame = grouped_by_frame[['text', 'frame']]\n",
    "# grouped_by_frame.to_csv('mfc_frames_for_cot.csv')\n",
    "grouped_by_frame = grouped_by_frame.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "411a523e-5ee1-416d-b62a-c4f736d1ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_frame.shape\n",
    "el_string = grouped_by_frame.to_json(orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "210001da-d2a0-4d81-84fb-03996489dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "small_sys_prompt = (\"You are a PhD-level expert in political communication and media framing.\\n\"\n",
    "    \"Your task is to classify the **dominant frame** expressed in each piece of media text, using the 15-frame schema derived from Card et al. (2022) and Boydstun et al. (2014).\\n\\n\"\n",
    "                   )\n",
    "el_string = \"\"\n",
    "system_prompt = (\n",
    "    \"These frames represent common thematic lenses used in U.S. political discourse.\"\n",
    "    \"---\"\n",
    "    \"## Standard Media Frames and Codes\"\n",
    "    \"Each frame has a numeric frame_code (1–15) and a label. Choose only one dominant frame per text.\"\n",
    "    f\"{y_labels__string}\"\n",
    "    \"---\"\n",
    "    \"## Framing Guidelines\"\n",
    "    \"- Assign the **dominant frame** that best captures the main **logic**, **emphasis**, or **metaphor** of the piece.\\n\"\n",
    "    \"- Do **not** rely on surface keywords. Focus on the **underlying argument** or lens.\\n\"\n",
    "    \"- If no frame adequately captures the text, assign frame_code `15` (\\\"Other\\\") and propose:\\n\"\n",
    "    \"  - `new_frame_code`: a descriptive label\\n\"\n",
    "    \"  - `rationale`: 1–2 sentence explanation\\n\\n\"\n",
    "    \"---\\n\\n\"\n",
    "    \"## Few-Shot Examples\\n\\n\"\n",
    "    \"Use these human-coded examples to guide your classification:\\n\\n\"\n",
    "    f\"{el_string}\\n\\n\"\n",
    "    \"---\\n\\n\"\n",
    "    \"## Output Format\\n\\n\"\n",
    "    \"Return results in **valid JSON**, as a list of objects—one per text. Each object must include:\\n\"\n",
    "    \"- `id`: the numeric identifier of the text\\n\"\n",
    "    \"- `frame`: the most appropriate standard frame label from the list\\n\\n\"\n",
    "    \"Return your result as a JSON array enclosed in three backticks, one object per text, like this:\\n\"\n",
    "    \"``` \\n\"\n",
    "    \"[\\\\{\\\"id\\\": 1, \\\"frame\\\": ECONOMIC\\\\}, ...]\\n\"\n",
    "    \"```\\n\\n\"\n",
    "    \"### Text to analyze:\\n  ##ROWS##\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "654e0cf2-4104-413c-aaef-06ce44482d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/25 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 0 and 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   4%|▍         | 1/25 [00:05<02:10,  5.45s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 25 and 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   8%|▊         | 2/25 [00:10<01:58,  5.17s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 50 and 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  12%|█▏        | 3/25 [00:16<01:58,  5.38s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 75 and 100\n",
      "⏳ Throttling... sleeping for 43.94s (retry 1/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  16%|█▌        | 4/25 [01:05<07:59, 22.84s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 100 and 125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  20%|██        | 5/25 [01:10<05:26, 16.35s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 125 and 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  24%|██▍       | 6/25 [01:15<03:58, 12.56s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 150 and 175\n",
      "⏳ Throttling... sleeping for 44.28s (retry 1/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  28%|██▊       | 7/25 [02:07<07:36, 25.38s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 175 and 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  32%|███▏      | 8/25 [02:13<05:25, 19.14s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 200 and 225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  36%|███▌      | 9/25 [02:18<03:58, 14.91s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 225 and 250\n",
      "⏳ Throttling... sleeping for 41.13s (retry 1/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  40%|████      | 10/25 [03:04<06:08, 24.54s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 250 and 275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  44%|████▍     | 11/25 [03:10<04:21, 18.69s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 275 and 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  48%|████▊     | 12/25 [03:15<03:10, 14.62s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 300 and 325\n",
      "⏳ Throttling... sleeping for 44.33s (retry 1/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  52%|█████▏    | 13/25 [04:14<05:34, 27.91s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 325 and 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  56%|█████▌    | 14/25 [04:19<03:51, 21.01s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 350 and 375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  60%|██████    | 15/25 [04:24<02:42, 16.30s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 375 and 400\n",
      "⏳ Throttling... sleeping for 35.39s (retry 1/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  64%|██████▍   | 16/25 [05:05<03:32, 23.56s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 400 and 425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  68%|██████▊   | 17/25 [05:10<02:24, 18.10s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 425 and 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  72%|███████▏  | 18/25 [05:15<01:39, 14.27s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 450 and 475\n",
      "⏳ Throttling... sleeping for 44.26s (retry 1/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  76%|███████▌  | 19/25 [06:06<02:30, 25.08s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 475 and 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  80%|████████  | 20/25 [06:12<01:36, 19.33s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 500 and 525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  84%|████████▍ | 21/25 [06:18<01:01, 15.42s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 525 and 550\n",
      "⏳ Throttling... sleeping for 41.77s (retry 1/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  88%|████████▊ | 22/25 [07:12<01:21, 27.04s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 550 and 575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  92%|█████████▏| 23/25 [07:18<00:41, 20.78s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 575 and 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  96%|█████████▌| 24/25 [07:23<00:16, 16.00s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch between idx 600 and 610\n",
      "⏳ Throttling... sleeping for 36.58s (retry 1/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 25/25 [08:02<00:00, 19.31s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration was 482759.00530815125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inx = 25\n",
    "# prompt = \"Please give the correct framing for the text. Take into account the examples you have, and then show your reasoning \n",
    "# for your decision before outputting the best frame in JSON, as instructed. \" + harvard_df.loc[inx,'text']\n",
    "\n",
    "sodiGPT2 = sodiGPT(system_prompt = small_sys_prompt)\n",
    "sodiGPT2.batch_size = 25\n",
    "sodiGPT2.prompt = system_prompt\n",
    "sodiGPT2.source_column_name = 'a'\n",
    "# sodiGPT2.source_column_name = 'text'\n",
    "sodiGPT2.target_column_llmresp = 'llm_response'\n",
    "sodiGPT2.target_columns = [\"gpt_frame\"]\n",
    "sodiGPT2.target_columns_in_json = [\"frame\"]\n",
    "sodiGPT2.pattern = \"##ROWS##\"\n",
    "\n",
    "\n",
    "# harvard_df.head()\n",
    "# gilardi_df = harvard_df[['Unnamed: 0','text', 'frame', 'gpt_frame']]\n",
    "# gilardi_df.rename(columns={'gpt_frame': 'gpt_frame_old', 'Unnamed: 0': 'uqidx'})\n",
    "\n",
    "\n",
    "# # fdf = gilardi_df.sample(50).copy()\n",
    "# fdf =gilardi_df.copy()\n",
    "fdf = df.copy()\n",
    "gdf, duration = sodiGPT2.process_df_chunked(fdf)\n",
    "gdf.head()\n",
    "\n",
    "gdf.to_csv('procon_dev_withChatGPTFrames.csv')\n",
    "print(f\"duration was {duration}\")\n",
    "\n",
    "# print(sodiGPT2.prompt)\n",
    "\n",
    "# resp = sodiGPT2.call(prompt)\n",
    "\n",
    "# # print(resp)\n",
    "# print(f\" the text {harvard_df.loc[inx,'text']} has {harvard_df.loc[inx,'frame']} but got labeled {resp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1c94e23c-91d9-4c6b-9f62-9530f70dad8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>frame</th>\n",
       "      <th>gpt_frame</th>\n",
       "      <th>frame_boydston</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"I don't want my state picked on. I don't want...</td>\n",
       "      <td>FAIRNESS AND EQUALITY</td>\n",
       "      <td>Cultural Identity</td>\n",
       "      <td>Fairness and Equality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Working people deserve #ABetterDeal than the o...</td>\n",
       "      <td>QUALITY OF LIFE</td>\n",
       "      <td>Economics</td>\n",
       "      <td>Quality of Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The Trump Administration has replaced one cris...</td>\n",
       "      <td>POLICY PRESCRIPTION AND EVALUATION</td>\n",
       "      <td>Morality And Ethics</td>\n",
       "      <td>Policy Description, Prescription, Evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>So proud of #VT National Guard units and the V...</td>\n",
       "      <td>SECURITY AND DEFENSE</td>\n",
       "      <td>Security And Defense</td>\n",
       "      <td>Security and Defense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Tragically, unaccompanied children in @HHSGov’...</td>\n",
       "      <td>HEALTH AND SAFETY</td>\n",
       "      <td>Crime And Punishment</td>\n",
       "      <td>Health and Safety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>124</td>\n",
       "      <td>We recognize May as #MilitaryAppreciationMonth...</td>\n",
       "      <td>SECURITY AND DEFENSE</td>\n",
       "      <td>Security And Defense</td>\n",
       "      <td>Security and Defense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>126</td>\n",
       "      <td>@EPAScottPruitt @AP Personally attacking a rep...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>Legality, Constitutionality, Jurisdiction</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>127</td>\n",
       "      <td>✈Acceleration of the safe integration of UAS t...</td>\n",
       "      <td>POLICY PRESCRIPTION AND EVALUATION</td>\n",
       "      <td>Legality, Constitutionality, Jurisdiction</td>\n",
       "      <td>Policy Description, Prescription, Evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>130</td>\n",
       "      <td>We've tried Trump #immigration strategy  befor...</td>\n",
       "      <td>POLICY PRESCRIPTION AND EVALUATION</td>\n",
       "      <td>Legality, Constitutionality, Jurisdiction</td>\n",
       "      <td>Policy Description, Prescription, Evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>131</td>\n",
       "      <td>Watch here: Forum on deported veterans with @R...</td>\n",
       "      <td>SECURITY AND DEFENSE</td>\n",
       "      <td>Legality, Constitutionality, Jurisdiction</td>\n",
       "      <td>Security and Defense</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                               text  \\\n",
       "0            0  \"I don't want my state picked on. I don't want...   \n",
       "1            1  Working people deserve #ABetterDeal than the o...   \n",
       "2            2  The Trump Administration has replaced one cris...   \n",
       "3            3  So proud of #VT National Guard units and the V...   \n",
       "4            4  Tragically, unaccompanied children in @HHSGov’...   \n",
       "..         ...                                                ...   \n",
       "95         124  We recognize May as #MilitaryAppreciationMonth...   \n",
       "96         126  @EPAScottPruitt @AP Personally attacking a rep...   \n",
       "97         127  ✈Acceleration of the safe integration of UAS t...   \n",
       "98         130  We've tried Trump #immigration strategy  befor...   \n",
       "99         131  Watch here: Forum on deported veterans with @R...   \n",
       "\n",
       "                                 frame  \\\n",
       "0                FAIRNESS AND EQUALITY   \n",
       "1                      QUALITY OF LIFE   \n",
       "2   POLICY PRESCRIPTION AND EVALUATION   \n",
       "3                 SECURITY AND DEFENSE   \n",
       "4                    HEALTH AND SAFETY   \n",
       "..                                 ...   \n",
       "95                SECURITY AND DEFENSE   \n",
       "96                               OTHER   \n",
       "97  POLICY PRESCRIPTION AND EVALUATION   \n",
       "98  POLICY PRESCRIPTION AND EVALUATION   \n",
       "99                SECURITY AND DEFENSE   \n",
       "\n",
       "                                    gpt_frame  \\\n",
       "0                           Cultural Identity   \n",
       "1                                   Economics   \n",
       "2                         Morality And Ethics   \n",
       "3                        Security And Defense   \n",
       "4                        Crime And Punishment   \n",
       "..                                        ...   \n",
       "95                       Security And Defense   \n",
       "96  Legality, Constitutionality, Jurisdiction   \n",
       "97  Legality, Constitutionality, Jurisdiction   \n",
       "98  Legality, Constitutionality, Jurisdiction   \n",
       "99  Legality, Constitutionality, Jurisdiction   \n",
       "\n",
       "                                  frame_boydston  \n",
       "0                          Fairness and Equality  \n",
       "1                                Quality of Life  \n",
       "2   Policy Description, Prescription, Evaluation  \n",
       "3                           Security and Defense  \n",
       "4                              Health and Safety  \n",
       "..                                           ...  \n",
       "95                          Security and Defense  \n",
       "96                                         Other  \n",
       "97  Policy Description, Prescription, Evaluation  \n",
       "98  Policy Description, Prescription, Evaluation  \n",
       "99                          Security and Defense  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# harvard_df.loc[5,'frame']\n",
    "# df.head()\n",
    "# sodiGPT2.source_column_name\n",
    "# sodiGPT2.system_prompt\n",
    "\n",
    "# def count_tokens(text, model=\"gpt-4o\"):\n",
    "#     enc = tiktoken.encoding_for_model(model)\n",
    "#     return len(enc.encode(text))\n",
    "\n",
    "\n",
    "# print(\"TOKEN COUNT:\", count_tokens(f\"\\n\\n{small_sys_prompt} \\n\\n {system_prompt}\"))\n",
    "# sodiGPT2.call_getResponse(system_prompt)\n",
    "\n",
    "# labels_df = pd.read_csv('label_matches.csv')\n",
    "# def get_boydstun_from_gilardi(gilardi):\n",
    "#     row = labels_df[labels_df['Original'] == gilardi]\n",
    "#     if row.shape[0] < 1:\n",
    "#         return \"None\"\n",
    "#     return row.iloc[0, labels_df.columns.get_loc('Boydston')]\n",
    "    \n",
    "# # # # gdf.head()\n",
    "# gdf['gpt_frame'] = gdf['gpt_frame'].str.title()\n",
    "# gdf['frame_boydston'] = gdf['frame'].apply(get_boydstun_from_gilardi)\n",
    "\n",
    "# eqdf = gdf[gdf['gpt_frame'].str.title() == gdf['frame_boydston']]\n",
    "# eqdf.shape\n",
    "\n",
    "\n",
    "# gdf.to_csv('gilardi_withimprovedChatGPTFrames.csv')\n",
    "\n",
    "# gdf.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4da2e558-9b41-4862-ab3c-dc8470ff53d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.3.0\n",
      "    Uninstalling mpmath-1.3.0:\n",
      "      Successfully uninstalled mpmath-1.3.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.6.0 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.13.3 which is incompatible.\n",
      "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.6.0 which is incompatible.\n",
      "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed mpmath-1.3.0 sympy-1.13.3\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentence_transformers\n",
    "# !pip install tqdm\n",
    "!pip install --upgrade --force-reinstall sympy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3cacd3e7-9d43-4c85-b235-f01b762bf2b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nmodule 'sympy' has no attribute 'printing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:1817\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/ihome/crc/install/pytorch/2.0.1/python3.10/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:41\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEntropyLoss, Identity\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m checkpoint\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_activation\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:15\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraceback\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfx_traceback\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_aot_autograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_fun\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree_map\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/functional_utils.py:20\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_subclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeta_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_sparse_any\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_shapes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     definitely_true,\n\u001b[1;32m     22\u001b[0m     sym_eq,\n\u001b[1;32m     23\u001b[0m     SymIntEqByExpr,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreductions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StorageWeakRef\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:74\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     75\u001b[0m     Application,\n\u001b[1;32m     76\u001b[0m     CeilToInt,\n\u001b[1;32m     77\u001b[0m     CleanDiv,\n\u001b[1;32m     78\u001b[0m     FloorDiv,\n\u001b[1;32m     79\u001b[0m     FloorToInt,\n\u001b[1;32m     80\u001b[0m     IsNonOverlappingAndDenseIndicator,\n\u001b[1;32m     81\u001b[0m     Mod,\n\u001b[1;32m     82\u001b[0m     PythonMod,\n\u001b[1;32m     83\u001b[0m )\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumbers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m int_oo\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_sympy/functions.py:185\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# It would be nice to have assertions on whether or not inputs is_integer\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# However, with bugs like https://github.com/sympy/sympy/issues/26620 sympy\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# sometimes inconsistently reports floats an integers.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Right now, FloorDiv de facto changes behavior if arguments are negative or\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# not, this can potentially cause correctness issues.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFloorDiv\u001b[39;00m(sympy\u001b[38;5;241m.\u001b[39mFunction):\n\u001b[1;32m    186\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    We maintain this so that:\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m    1. We can use divisibility guards to simplify FloorDiv(a, b) to a / b.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m    NB: This is Python-style floor division, round to -Inf\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_sympy/functions.py:206\u001b[0m, in \u001b[0;36mFloorDiv\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sympystr\u001b[39m(\u001b[38;5;28mself\u001b[39m, printer: \u001b[43msympy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprinting\u001b[49m\u001b[38;5;241m.\u001b[39mStrPrinter) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    207\u001b[0m     base \u001b[38;5;241m=\u001b[39m printer\u001b[38;5;241m.\u001b[39mparenthesize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase, PRECEDENCE[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAtom\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sympy' has no attribute 'printing'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/__init__.py:14\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[1;32m     11\u001b[0m     export_optimized_onnx_model,\n\u001b[1;32m     12\u001b[0m     export_static_quantized_openvino_model,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     CrossEncoder,\n\u001b[1;32m     16\u001b[0m     CrossEncoderModelCardData,\n\u001b[1;32m     17\u001b[0m     CrossEncoderTrainer,\n\u001b[1;32m     18\u001b[0m     CrossEncoderTrainingArguments,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainer\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautonotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trange\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     AutoConfig,\n\u001b[1;32m     17\u001b[0m     AutoModelForSequenceClassification,\n\u001b[1;32m     18\u001b[0m     AutoTokenizer,\n\u001b[1;32m     19\u001b[0m     PretrainedConfig,\n\u001b[1;32m     20\u001b[0m     PreTrainedModel,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:1805\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1803\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:1819\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1820\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1821\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1822\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nmodule 'sympy' has no attribute 'printing'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tqdm\n",
    "\n",
    "def get_frame_code_from_frame(text):\n",
    "\n",
    "    y_labels_semantic_order = {\n",
    "            1.0: \"Political\",\n",
    "            2.0: \"Public Sentiment\",\n",
    "            3.0: \"Cultural Identity\",\n",
    "            4.0: \"Morality and Ethics\",\n",
    "            5.0: \"Fairness and Equality\",\n",
    "            6.0: \"Legality, Constitutionality, Jurisdiction\",\n",
    "            7.0: \"Crime and Punishment\",\n",
    "            8.0: \"Security and Defense\",\n",
    "            9.0: \"Health and Safety\",\n",
    "            10.0: \"Quality of Life\",\n",
    "            11.0: \"Economics\",\n",
    "            12.0: \"Capacity and Resources\",\n",
    "            13.0: \"Policy Description, Prescription, Evaluation\",\n",
    "            14.0: \"External Regulation and Reputation\",\n",
    "            15.0: \"Other\"\n",
    "        }\n",
    "\n",
    "    for real in y_labels_semantic_order:\n",
    "        if text.title() == y_labels_semantic_order[real]:\n",
    "            return real\n",
    "\n",
    "    return -1.0\n",
    "\n",
    "idf = pd.read_csv('procon_dev_withChatGPTFrames.csv')\n",
    "pca_model = SentenceTransformer(\"all-MiniLM-L6-v2\") \n",
    "idf['embedding'] = idf['a'].apply(lambda x: pca_model.encode(x))\n",
    "idf['frame_code'] = gdf.apply(lambda row: get_frame_code_from_frame(row['gpt_frame']), axis=1)\n",
    "ldf[\"y_offset\"] = np.nan\n",
    "\n",
    "# Loop through groups and compute offsets\n",
    "for code, group in idf.groupby(\"gpt_frame\"):\n",
    "    try:\n",
    "        embeddings = np.vstack(group[\"embedding\"].values)\n",
    "\n",
    "        if len(embeddings) > 1:\n",
    "            pca = PCA(n_components=1)\n",
    "            reduced = pca.fit_transform(embeddings)\n",
    "\n",
    "            scaler = MinMaxScaler(feature_range=(-5, 5))\n",
    "            normalized = scaler.fit_transform(reduced).flatten()\n",
    "        else:\n",
    "            normalized = np.array([0.0])  # Single entry gets 0\n",
    "\n",
    "        idf.loc[group.index, \"y_offset\"] = normalized\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping group {code} due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "idf.drop(columns=[\"embedding\"], inplace=True)\n",
    "idf['y'] = gdf.apply(lambda row: float(row['frame_code']) + float(row['y_offset']), axis=1)\n",
    "ldf.to_csv(\"procon_dev_withChatGPTFrames_for_graphing.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870cac44-ce9b-40ff-99df-bea267536fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
